{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d9eed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import gzip, shutil\n",
    "import hashlib\n",
    "\n",
    "from six.moves.urllib.error import HTTPError \n",
    "from six.moves.urllib.error import URLError\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "import os\n",
    "# The functions used in this file to download the dataset are based on \n",
    "# code from the keras library. Specifically, from the following file:\n",
    "# https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/utils/data_utils.py\n",
    "\n",
    "def _hash_file(fpath, algorithm='sha256', chunk_size=65535):\n",
    "    if (algorithm == 'sha256') or (algorithm == 'auto' and len(hash) == 64):\n",
    "        hasher = hashlib.sha256()\n",
    "    else:\n",
    "        hasher = hashlib.md5()\n",
    "\n",
    "    with open(fpath, 'rb') as fpath_file:\n",
    "        for chunk in iter(lambda: fpath_file.read(chunk_size), b''):\n",
    "            hasher.update(chunk)\n",
    "\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "def validate_file(fpath, file_hash, algorithm='auto', chunk_size=65535):\n",
    "    if (algorithm == 'sha256') or (algorithm == 'auto' and len(file_hash) == 64):\n",
    "        hasher = 'sha256'\n",
    "    else:\n",
    "        hasher = 'md5'\n",
    "\n",
    "    if str(_hash_file(fpath, hasher, chunk_size)) == str(file_hash):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_file(fname,\n",
    "             origin,\n",
    "             md5_hash=None,\n",
    "             file_hash=None,\n",
    "             cache_subdir='datasets',\n",
    "             hash_algorithm='auto',\n",
    "             extract=False,\n",
    "             archive_format='auto',\n",
    "             cache_dir=None):\n",
    "    if cache_dir is None:\n",
    "        cache_dir = os.path.join(os.path.expanduser('~'), '.data-cache')\n",
    "    if md5_hash is not None and file_hash is None:\n",
    "        file_hash = md5_hash\n",
    "        hash_algorithm = 'md5'\n",
    "    datadir_base = os.path.expanduser(cache_dir)\n",
    "    if not os.access(datadir_base, os.W_OK):\n",
    "        datadir_base = os.path.join('/tmp', '.data-cache')\n",
    "    datadir = os.path.join(datadir_base, cache_subdir)\n",
    "\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    os.makedirs(datadir, exist_ok=True)\n",
    "\n",
    "    fpath = os.path.join(datadir, fname)\n",
    "\n",
    "    download = False\n",
    "    if os.path.exists(fpath):\n",
    "    # File found; verify integrity if a hash was provided.\n",
    "        if file_hash is not None:\n",
    "            if not validate_file(fpath, file_hash, algorithm=hash_algorithm):\n",
    "                print('A local file was found, but it seems to be '\n",
    "                      'incomplete or outdated because the ' + hash_algorithm +\n",
    "                      ' file hash does not match the original value of ' + file_hash +\n",
    "                      ' so we will re-download the data.')\n",
    "                download = True\n",
    "    else:\n",
    "        download = True\n",
    "\n",
    "    if download:\n",
    "        print('Downloading data from', origin)\n",
    "\n",
    "        error_msg = 'URL fetch failure on {}: {} -- {}'\n",
    "        try:\n",
    "            try:\n",
    "                urlretrieve(origin, fpath)\n",
    "            except HTTPError as e:\n",
    "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
    "            except URLError as e:\n",
    "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
    "        except (Exception, KeyboardInterrupt) as e:\n",
    "            if os.path.exists(fpath):\n",
    "                os.remove(fpath)\n",
    "\n",
    "    return fpath\n",
    "\n",
    "def get_and_gunzip(origin, filename, md5hash=None, cache_dir=None, cache_subdir=None):\n",
    "    gz_file_path = get_file(filename, origin, md5_hash=md5hash, cache_dir=cache_dir, cache_subdir=cache_subdir)\n",
    "    hdf5_file_path = gz_file_path[:-3]\n",
    "    if not os.path.isfile(hdf5_file_path) or os.path.getctime(gz_file_path) > os.path.getctime(hdf5_file_path):\n",
    "        print(\"Decompressing %s\"%gz_file_path)\n",
    "        with gzip.open(gz_file_path, 'r') as f_in, open(hdf5_file_path, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "    return hdf5_file_path\n",
    "\n",
    "def get_shd_dataset(cache_dir, cache_subdir):\n",
    "\n",
    "    # The remote directory with the data files\n",
    "    base_url = \"https://zenkelab.org/datasets\"\n",
    "\n",
    "    # Retrieve MD5 hashes from remote\n",
    "    response = urllib.request.urlopen(\"%s/md5sums.txt\"%base_url)\n",
    "    data = response.read() \n",
    "    lines = data.decode('utf-8').split(\"\\n\")\n",
    "    file_hashes = { line.split()[1]:line.split()[0] for line in lines if len(line.split())==2 }\n",
    "\n",
    "    # Download the Spiking Heidelberg Digits (SHD) dataset\n",
    "    files = [ \"shd_train.h5.gz\", \n",
    "              \"shd_test.h5.gz\",\n",
    "            ]\n",
    "    for fn in files:\n",
    "        origin = \"%s/%s\"%(base_url,fn)\n",
    "        hdf5_file_path = get_and_gunzip(origin, fn, md5hash=file_hashes[fn], cache_dir=cache_dir, cache_subdir=cache_subdir)\n",
    "        # print(\"File %s decompressed to:\"%(fn))\n",
    "        print(\"Available at: %s\"%hdf5_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430ed7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://zenkelab.org/datasets/shd_train.h5.gz\n",
      "Decompressing /tmp\\.data-cache\\hdspikes\\shd_train.h5.gz\n",
      "Available at: /tmp\\.data-cache\\hdspikes\\shd_train.h5\n",
      "Downloading data from https://zenkelab.org/datasets/shd_test.h5.gz\n",
      "Decompressing C:\\Users\\Mark/data\\hdspikes\\shd_test.h5.gz\n",
      "Available at: C:\\Users\\Mark/data\\hdspikes\\shd_test.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from util.utils import get_shd_dataset\n",
    "\n",
    "cache_dir = os.path.expanduser(\"~/data\")\n",
    "cache_subdir = \"hdspikes\"\n",
    "get_shd_dataset(cache_dir, cache_subdir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whk-snn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
